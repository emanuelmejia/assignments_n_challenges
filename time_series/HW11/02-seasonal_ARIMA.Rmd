# (10 points) Seasonal ARIMA model

Download the series of E-Commerce Retail Sales as a Percent of Total Sales [here](https://fred.stlouisfed.org/series/ECOMPCTNSA). 

(Feel free to explore the `fredr` package and API if interested.)

Our goal is to Build a Seasonal ARIMA model, following all appropriate steps for a univariate time series model.

Separate the data set into training and test data. The training data is used to estimate model parameters, and it is for 10/1999-12/2020. The test data is used to evaluate its accuracy, and it is for 01/2021-01/22.

```{r load libraries 2, results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(fredr)
library(tidyverse)
library(feasts)
library(magrittr)
library(patchwork)
library(tsibble)
library(forecast)
library(lmtest)
library(lubridate)
library(fable)
library(tseries)
library(reshape)
fredr_set_key("5aec2211891f54292c669f91c2c03110")
```

```{r creating ecom datasets}
# Downloading dataset from FREDR data
ecom <- fredr(
  series_id = "ECOMPCTNSA"
  )


ecom.ts <- ecom %>%
  mutate(time_index = yearquarter(date)) %>% 
  as_tsibble(index = time_index)

ecom.train <- ecom.ts %>% 
  filter_index(~ "2020-12-31")

ecom.test <- ecom.ts %>% 
  filter_index("2021-01-01" ~.)
```

```{r graph functions}
# Graph functions to easily repeat plots

# TS line plot
gglineplot <- function(ts = NULL,       # Time Series to be used
                   var = NULL,          # Variable to be plotted
                   graph_title = "",    # Title for the graph
                   graph_subtitle = "", # Subtitle for the graph
                   x_label = "",        # Custom x axis label
                   y_label = ""         # Custom x axis label
                   ) {
  
  graph <- ggplot(
    ts, 
    var
    ) +
    geom_line(
      size = 0.8,
      color = "cornflowerblue"
      ) +
    labs(
      title = graph_title,
      subtitle = graph_subtitle,
      x = x_label,
      y = y_label
      ) +
      theme_classic() +
      theme(
      plot.title = element_text(color = "#0099F8",
                                size = 20,
                                face = "bold"),
      plot.subtitle = element_text(color="#969696",
                                   size = 14,
                                   face = "italic"),
      axis.title = element_text(color = "#969696",
                                size = 12,
                                face = "bold"),
      axis.text = element_text(color = "#969696", size = 11),
      axis.line = element_line(color = "#969696"),
      axis.ticks = element_line(color = "#969696"),
      ) + scale_y_continuous(labels = scales::percent)
    
  return (graph)
}

# TS ACF plot
ggacfplot <- function(ts = NULL,           # Time Series to be used
                     var = NULL,           # Variable to be plotted
                     graph_subtitle = "",  # Subtitle for the graph
                     x_label = "",         # Custom x axis label
                     y_label = ""          # Custom x axis label
                     ) {
  
  acfgraph <- ACF(
    ts,
    var
    ) %>% autoplot() +
      labs(
          title = "Autocorrelation Function",
          subtitle = graph_subtitle,
          x = x_label,
          y = "Autocorrelation"
        ) +
      theme_classic() +
      theme(
      plot.title = element_text(color = "#0099F8",
                                size = 40,
                                face = "bold"),
      plot.subtitle = element_text(color="#969696",
                                   size = 32,
                                   face = "italic"),
      axis.title = element_text(color = "#969696",
                                size = 28,
                                face = "bold"),
      axis.text = element_text(color = "#969696", size = 28),
      axis.line = element_line(color = "#969696"),
      axis.ticks = element_line(color = "#969696")
      ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))
  
  return (acfgraph)
}

#TS PACF plot
ggpacfplot <- function(ts = NULL,           # Time Series to be used
                     var = NULL,            # Variable to be plotted
                     graph_subtitle = "",   # Subtitle for the graph
                     x_label = "",          # Custom x axis label
                     y_label = ""           # Custom x axis label
                     ) {
  
  pacfgraph <- PACF(
    ts,
    var
    ) %>% autoplot() +
      labs(
          title = "Partial Autocorrelation Function",
          subtitle = graph_subtitle,
          x = x_label,
          y = "Partial Autocorrelation"
        ) +
      theme_classic() +
      theme(
      plot.title = element_text(color = "#0099F8",
                                size = 40,
                                face = "bold"),
      plot.subtitle = element_text(color="#969696",
                                   size = 32,
                                   face = "italic"),
      axis.title = element_text(color = "#969696",
                                size = 28,
                                face = "bold"),
      axis.text = element_text(color = "#969696", size = 28),
      axis.line = element_line(color = "#969696"),
      axis.ticks = element_line(color = "#969696")
      ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))
  
  return (pacfgraph)
}
```

## Time series plot

Plot training data set of Retail Sales. What do you notice? Is there any transformation necessary?

```{r ecom time series plot}
gglineplot(
  ts = ecom.train,
  var = aes(x = time_index, y = value / 100),
  graph_title = "E-Commerce Retail Sales",
  graph_subtitle = "As a Percent of Total Sales",
  x_label = "Date",
  y_label = "E-Commerce Retail Sales (%)"
)
```

> Data has a pronounced positive trend and also the variance seems to increase as time passes. We'll likely need to difference the data in order to make it stationary.

## Check for Stationary 

Use ACF/PACF and a unit root test to check if Retail Sales is stationary. If data is not stationary, difference the data, and apply the
test again until it becomes stationary? How many differences are needed to make data stationary?

```{r Plots Original, warning = FALSE, fig.width=20, fig.height=7}
# ACF - Original series
ecom.acf <- ggacfplot(
  ts = ecom.train,
  var = ecom.train$value,
  graph_subtitle = "Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

# PACF - Original series
ecom.pacf <- ggpacfplot(
  ts = ecom.train,
  var = ecom.train$value,
  graph_subtitle = "Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

ecom.acf | ecom.pacf

# Unit Root Tests
PP.test(ecom.train$value)

adf.test(ecom.train$value, alternative = "stationary", k = 1)
adf.test(ecom.train$value, alternative = "stationary", k = 2)
```
> When applying the PP test and the ADF test (up to 2 lags) to the original series we fail to reject the null hypothesis meaning that we might still have a unit root and the series is non stationary. Which is also consistent when looking at the ACF and PACF plot (which might also suggest at least an AR(1) since the PACF for lag 1 is really high).

```{r Plots 1st Difference, warning = FALSE, fig.width=20, fig.height=7}
# ACF - 1st Difference Series
ecom.diff.acf <- ggacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 1),
  graph_subtitle = "1st Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

# PACF - 1st Difference Series
ecom.diff.pacf <- ggpacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 1),
  graph_subtitle = "1st Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

ecom.diff.acf | ecom.diff.pacf

# Unit Root Tests
PP.test(diff(ecom.train$value, differences = 1))

adf.test(diff(ecom.train$value, differences = 1), alternative = "stationary", k = 1)
adf.test(diff(ecom.train$value, differences = 1), alternative = "stationary", k = 2)
adf.test(diff(ecom.train$value, differences = 1), alternative = "stationary", k = 3)
adf.test(diff(ecom.train$value, differences = 1), alternative = "stationary", k = 4)
```
> When applying the PP test and the ADF test (up to 2 lags) and to the 1st differenced series we reject the null hypothesis ($H_0 : \text{not stationary}$) meaning that this series might be stationary, although when trying for lags 3 and 4 for the ADF test we fail to reject the null and by looking at the ACF and PACF graphs it doesn't seem stationary yet

```{r Plots 2nd Difference, warning = FALSE, fig.width=20, fig.height=7}
# ACF - 2nd Difference Series
ecom.diff2.acf <- ggacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 2),
  graph_subtitle = "2nd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

# PACF - 2nd Difference Series
ecom.diff2.pacf <- ggpacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 2),
  graph_subtitle = "2nd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

ecom.diff2.acf | ecom.diff2.pacf

# Unit Root Tests
PP.test(diff(ecom.train$value, differences = 2))

adf.test(diff(ecom.train$value, differences = 2), alternative = "stationary", k = 1)
adf.test(diff(ecom.train$value, differences = 2), alternative = "stationary", k = 2)
adf.test(diff(ecom.train$value, differences = 2), alternative = "stationary", k = 3)
adf.test(diff(ecom.train$value, differences = 2), alternative = "stationary", k = 4)
```
> When applying the PP test and ADF test for the 2nd differenced series, we reject the null hypothesis ($H_0 : \text{not stationary}$) meaning that this series might be stationary as well even for higher lags (up to 4), but the the ACF and PACF graphs still have many significant values which says again that perhaps we're dealing with an AR model and a seasonal component as well.

```{r Plots 3rd Difference, warning = FALSE, fig.width=20, fig.height=7}
# ACF - 3rd Difference Series
ecom.diff3.acf <- ggacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 3),
  graph_subtitle = "3rd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

# PACF - 3rd Difference Series
ecom.diff3.pacf <- ggpacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 3),
  graph_subtitle = "3rd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

ecom.diff3.acf | ecom.diff3.pacf

# Unit Root Tests
PP.test(diff(ecom.train$value, differences = 3))

adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 1)
adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 2)
adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 3)
adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 4)
adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 5)
adf.test(diff(ecom.train$value, differences = 3), alternative = "stationary", k = 6)
```
> When applying the PP test and ADF test (up to 6 lags) we fail to reject the null meaning that this series might also be stationary, but again the the ACF and PACF graphs have many significant values and it seems like we're really not getting significantly better results . We'll still try another iteration (since lag 4 would mean 1 year back).

```{r Plots 4th Difference, warning = FALSE, fig.width=20, fig.height=7}
# ACF - 4th Difference Series
ecom.diff4.acf <- ggacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 4),
  graph_subtitle = "3rd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

# PACF - 4th Difference Series
ecom.diff4.pacf <- ggpacfplot(
  ts = ecom.train,
  var = diff(ecom.train$value, differences = 4),
  graph_subtitle = "3rd Difference Ecommerce Series",
  x_label = "lag [Unit = 1Q]",
)

ecom.diff4.acf | ecom.diff4.pacf

# Unit Root Tests
PP.test(diff(ecom.train$value, differences = 4))

adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 1)
adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 2)
adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 3)
adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 4)
adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 5)
adf.test(diff(ecom.train$value, differences = 4), alternative = "stationary", k = 6)
```
> We're getting similar results, so we'll stop there. We would say that we can get a "stationary" series even from the first difference, but we would still need to combine it with an AR model and perhaps a seasonal component.

## Model identification and estimation

Use ACF/PACF to identify an appropriate SARIMA model. Estimate both select model and model chosen by ARIMA()

```{r Model identification and estimation}
# Human (myself) Selected Model
mod.select <- ecom.train %>%
  model(
    ARIMA(value ~ 1 + pdq(3,1,0) + PDQ(0,0,0), 
          ic="bic", 
          stepwise=F, 
          greedy=F)
    )

mod.select %>% report()

# ARIMA automatic selection
mod.arima <- ecom.train %>%
  model(
    ARIMA(value ~ 1 + pdq(0:10,0:10,0:10) + PDQ(0:10,0:10,0:10), 
          ic="bic", 
          stepwise=F, 
          greedy=F)
    )

mod.arima %>% report()
```

> The model I've picked is an ARIMA(3,1,0) due to the plots and analysis made before. We get a stationary series for the first difference but the PACF still has significant values for several lags, although I stopped at 3 lags for the AR process since adding one more would just mean going one year back. 

> Although when letting the function pick a model automatically it selects an ARIMA(1,0,0)(0,1,0)[4] which says that indeed we had an AR component but the difference is better applied in the seasonal component (specifically for lag 4 which is exactly one year before). The criterions confirm that indeed this seems like a better fitted model than the one I've picked.

## Model diagnostic 

Do residual diagnostic checking of both models. Are the residuals white noise? Use the Ljung-box test to check if the residuals are white noise.  

```{r Chosen Model Residuals, fig.width=20, fig.height=7}
# Residual Analysis for Human Selected Model
resid.select <- mod.select %>%
  augment() %>%
  dplyr::select(.resid) %>%
  as.ts()

select.acf <- mod.select %>%
  augment() %>%
  ACF(.resid) %>%
  autoplot()+ labs(
    x = "lag [Unit = 1Q]",
    y = "Autocorrelation",
    title = "Autocorrelation Function",
    subtitle = "Residuals - ARIMA(3,1,0) Human Chosen Model"
  ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 40,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 20,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 28,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 28),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696")
  ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))

select.pacf <- mod.select %>%
  augment() %>%
  PACF(.resid) %>%
  autoplot()+ labs(
    x = "lag [Unit = 1Q]",
    y = "Partial Autocorrelation",
    title = "Partial Autocorrelation Function",
    subtitle = "Residuals - ARIMA(3,1,0) Human Chosen Model"
  ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 40,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 20,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 28,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 28),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696")
  ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))

select.acf | select.pacf

Box.test(resid.select, lag=1, type="Ljung-Box")
Box.test(resid.select, lag=2, type="Ljung-Box")
Box.test(resid.select, lag=3, type="Ljung-Box")
Box.test(resid.select, lag=4, type="Ljung-Box")
```
```{r Automatic Model Residuals, fig.width=20, fig.height=7}
# Residual Analysis for ARIMA Selected Model
resid.arima <- mod.arima %>%
  augment() %>%
  dplyr::select(.resid) %>%
  as.ts()

arima.acf <- mod.arima %>%
  augment() %>%
  ACF(.resid) %>%
  autoplot()+ labs(
    x = "lag [Unit = 1Q]",
    y = "Autocorrelation",
    title = "Autocorrelation Function",
    subtitle = "Residuals - ARIMA(1,0,0)(0,1,0)[4] Automatic Chosen Model"
  ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 40,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 20,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 28,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 28),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696")
  ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))

arima.pacf <- mod.arima %>%
  augment() %>%
  PACF(.resid) %>%
  autoplot()+ labs(
    x = "lag [Unit = 1Q]",
    y = "Partial Autocorrelation",
    title = "Partial Autocorrelation Function",
    subtitle = "Residuals - ARIMA(1,0,0)(0,1,0)[4] Automatic Chosen Model"
  ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 40,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 20,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 28,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 28),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696")
  ) + scale_x_continuous(breaks = c(0,4,8,12,16,20))

arima.acf | arima.pacf

Box.test(resid.arima, lag=1, type="Ljung-Box")
Box.test(resid.arima, lag=2, type="Ljung-Box")
Box.test(resid.arima, lag=3, type="Ljung-Box")
Box.test(resid.arima, lag=4, type="Ljung-Box")
```

> For the model I've chosen - ARIMA(3,1,0) - the ACF and PACF don't show white noise at all and when performing the Ljung Box Tests we reject the null ($H_0: \text{data are independently distributed}$) up to 4 lags.

> On the other hand when looking at the automatically chosen model - ARIMA(1,0,0)(0,1,0)[4] even when it doesn't look completely as white noise there are no significant lags in either the ACF nor PACF, and when performing the Ljung Box Tests we fail to reject the null for every one of the first 4 lags, meaning these residuals might resemble a white noise process.


## Forecasting 

Use the both models to forecast the next 12 months and evaluate the forecast accuracy of these models.

```{r forecasting ecommerce}
# Forecasting 4 quarters ahead (12 Months)
per = 4

# Forecast Plot - Human Selected Model
sel.fore <- forecast(mod.select, h = per)
autoplot(
  ecom.ts, 
  .vars = value, 
  colour = "gray"
  ) +
  autolayer(sel.fore, colour="blue", alpha = 0.5) +
  labs(
      title = "E-Commerce Retail Sales",
      subtitle = "compared to ARIMA(3,1,0) Forecast",
      x = "Date",
      y = "E-Commerce Retail Sales (%)"
    ) +
    theme_classic() +
    theme(
    plot.title = element_text(color = "#0099F8",
                              size = 20,
                              face = "bold"),
    plot.subtitle = element_text(color="#969696",
                                 size = 14,
                                 face = "italic"),
    axis.title = element_text(color = "#969696",
                              size = 12,
                              face = "bold"),
    axis.text = element_text(color = "#969696", size = 11),
    axis.line = element_line(color = "#969696"),
    axis.ticks = element_line(color = "#969696"),
    legend.position = "none"
    )

# Accuracy Metrics - Human Selection
sel.eval <- accuracy(sel.fore, ecom.test)
sel.eval

# Forecast Plot - ARIMA Selected Model
ari.fore <- forecast(mod.arima, h = per)
autoplot(
  ecom.ts, 
  .vars = value, 
  colour = "gray"
  ) +
  autolayer(ari.fore, colour="blue", alpha = 0.35) +
  labs(
      title = "E-Commerce Retail Sales",
      subtitle = "compared to ARIMA(1,0,0)(0,1,0)[4] Forecast",
      x = "Date",
      y = "E-Commerce Retail Sales (%)"
  ) +
  theme_classic() +
  theme(
  plot.title = element_text(color = "#0099F8",
                            size = 20,
                            face = "bold"),
  plot.subtitle = element_text(color="#969696",
                               size = 14,
                               face = "italic"),
  axis.title = element_text(color = "#969696",
                            size = 12,
                            face = "bold"),
  axis.text = element_text(color = "#969696", size = 11),
  axis.line = element_line(color = "#969696"),
  axis.ticks = element_line(color = "#969696"),
  legend.position = "none"
  )

# Accuracy Metrics - ARIMA selection
ari.eval <- accuracy(ari.fore, ecom.test)
ari.eval
```
| Metric  | Human Chosen Model          | Automatically Chosen Model  |
|---------|-----------------------------|-----------------------------|
| RMSE    | `r round(sel.eval$RMSE,2)`  | `r round(ari.eval$RMSE,2)`  |
| MAE     | `r round(sel.eval$MAE,2)`   | `r round(ari.eval$MAE,2)`   |
| MAPE    | `r round(sel.eval$MAPE,2)`  | `r round(ari.eval$MAPE,2)`  |

> Even when the automatically chosen model (ARIMA(1,0,0)(0,1,0)[4]) performed better in the training data, by comparing the forecasts and the evaluation metrics, the model I've picked (ARIMA(3,1,0)) proved a better performance in test data. Although this might seem unfair because we can notice in the graph that the forecasted year didn't really follow the pattern we were observing before, so it wasn't really a good predictor, it was actually just luck.