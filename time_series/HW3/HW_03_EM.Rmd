---
title : 'W271 Assignment 3'
author: "Emanuel Mej√≠a"
output: 
  pdf_document:
    toc: true
    number_sections: true
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{enumitem}
   - \usepackage{rotating, graphicx}
   - \setlistdepth{99}
---

```{r load packages, message=FALSE}
library(tidyverse)
library(stargazer)
library(car)
```

# Customer churn study: **Part-3** (100 Points)

In the last two homework assignments, you initiated modeling a binary variable and used logistic regression to study the churn tendencies of customers.

Now, in Part-3, we're going to explore different interactions, transformations, and categorical explanatory variables to create a more comprehensive model.

```{r load of the data}
telcom_churn <- read.csv("./data/Telco_Customer_Churn.csv", header=T,na.strings=c("","NA")) 
```


For the remainder of this section, pay particular attention to all variables. 

## Data Preprocessing (5 Points)

In this section, Convert variables as needed, and manage any missing values.

We'll convert the following variables and treat categorical explanatory variables as factors.

```{r}
telcom_churn["Churn"][telcom_churn["Churn"] == "No"] <- 0
telcom_churn["Churn"][telcom_churn["Churn"] == "Yes"] <- 1
telcom_churn["SeniorCitizen"][telcom_churn["SeniorCitizen"] == 0] <- "No"
telcom_churn["SeniorCitizen"][telcom_churn["SeniorCitizen"] == 1] <- "Yes"

telcom_churn$Churn <- as.integer(telcom_churn$Churn)
telcom_churn$SeniorCitizen <- as.factor(telcom_churn$SeniorCitizen)
telcom_churn$gender <- as.factor(telcom_churn$gender)

```
Finally, we'll omit all the NA values since we have enought data, even without them.

```{r}
telcom_churn <- na.omit(telcom_churn)
```


## Estimate a logistic regression (10 Points)

Estimate the following binary logistic regressions and report the results in a table using stargazer package.

$$
  \begin{aligned}
    Churn = \beta_{0} & + \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen +\beta_{5} gender +  e \quad \text{(Model 1)} \\ 
    Churn = \beta_{0} & + \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen +\beta_{5} gender \quad \quad \quad\text{(Model 2)} \\       & + \beta_{6} tenure^2 + \beta_{7} MonthlyCharges^2 + \beta_{8} TotalCharges^2 + e\\
    Churn = \beta_{0} &+ \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen +\beta_{5} gender \quad  \quad \quad \text{(Model 3)} \\ 
      & +  \beta_{6} tenure^2 + \beta_{7} MonthlyCharges^2 + \beta_{8} TotalCharges^2 \\ 
      & + {\beta}_9 SeniorCitizen \times tenure + {\beta}_{10} SeniorCitizen \times MonthlyCharges \\ 
      & + \beta_{11} SeniorCitizen \times TotalCharges+ {\beta}_{12} gender \times tenure \\
      & + {\beta}_{13} gender \times MonthlyCharges + \beta_{14} gender \times TotalCharges      + e
  \end{aligned}
$$
- where  $SeniorCitizen \times MonthlyCharges$ denotes the interaction between `SeniorCitizen` and `MonthlyCharges` variables.

So we have the following 3 models:

```{r}
mod.fit1 <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen + gender, 
  family = binomial(link = logit), 
  data = telcom_churn
)

mod.fit2 <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen + gender + 
    I(tenure^2) + I(MonthlyCharges^2) + I(TotalCharges^2), 
  family = binomial(link = logit), 
  data = telcom_churn
)

mod.fit3 <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen + gender + 
    I(tenure^2) + I(MonthlyCharges^2) + I(TotalCharges^2) + SeniorCitizen:tenure + SeniorCitizen:MonthlyCharges + 
    SeniorCitizen:TotalCharges + gender:tenure + gender:MonthlyCharges + gender:TotalCharges, 
  family = binomial(link = logit), 
  data = telcom_churn
)
```

```{r Models results, warning=FALSE, render=FALSE, echo=FALSE, error=FALSE, message=FALSE, results='asis'}

stargazer(
 mod.fit1,
 mod.fit2,
 mod.fit3,
 title = "Results",
 type = 'latex',
 header=FALSE,
 align=TRUE,
 order="Constant",
 #  no.space=TRUE,
 covariate.labels = c("Intercept", "Tenure", "MonthlyCharges", "TotalCharges", "SeniorCitizen(Yes)", "Gender(Male)", "Tenure**2", "MonthlyCharges**2", "TotalCharges**2",
                      "Inter: Tenure | SeniorCitizen(Yes)", "Inter: MonthlyCharges | SeniorCitizen(Yes)", "Inter: TotalCharges | SeniorCitizen(Yes)",
                      "Inter: Tenure | Gender(Male)", "Inter: MonthlyCharges | Gender(Male)", "Inter: TotalCharges | Gender(Male)"
                      )
)

```


## Test a hypothesis: linear effects (15 Points)

Using `Model 1`, test the hypothesis of linear effects of variables on customer churn using a likelihood ratio test.

So we have the following:
$$
  \begin{aligned}
    Model_1 : Churn = \beta_{0} & + \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen +\beta_{5} gender +  e\\
  \end{aligned}
$$

And we'll perform an Anova Test to test each variable's effect on customer churn using LRT:

```{r Model1_LRT}
Anova.tests <- Anova(mod.fit1, test = "LR")
LR.chisq <- Anova.tests[["LR Chisq"]]
p.val.chisq <- Anova.tests[["Pr(>Chisq)"]]
Anova.tests
```

The results are as follows:

- For the test of `tenure` with $H_0: \beta_1 = 0$ VS $H_a: \beta_1 \ne 0$ we obtain $-2log(\Delta) =$ `r round(LR.chisq[1],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq[1],4)`$)=$ `r p.val.chisq[1]` meaning that we have a strong evidence that `tenure` is important to include in the model (given that the other variables are in the model as well).
- For the test of `MonthlyCharges` with $H_0: \beta_2 = 0$ VS $H_a: \beta_2 \ne 0$ we obtain $-2log(\Delta) =$ `r round(LR.chisq[2],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq[2],4)`$)=$ `r p.val.chisq[2]` meaning that we have strong evidence that `MonthlyCharges` is important to include in the model (given that the other variables are in the model as well).
- For the test of `TotalCharges` with $H_0: \beta_3 = 0$ VS $H_a: \beta_3 \ne 0$ we obtain $-2log(\Delta) =$ `r round(LR.chisq[3],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq[3],4)`$)=$ `r p.val.chisq[3]` meaning that we have marginal evidence that `TotalCharges` is important to include in the model (given that the other variables are in the model as well). So, depending on the confidence level we select we might include this variable or not (with $\alpha = 0.05$ we would include it, and we wouldn't include it if using $\alpha = 0.01$).
- For the test of `SeniorCitizen` with $H_0: \beta_4 = 0$ VS $H_a: \beta_4 \ne 0$ we obtain $-2log(\Delta) =$ `r round(LR.chisq[4],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq[4],4)`$)=$ `r p.val.chisq[4]` meaning that we have strong evidence that `Senior` is important to include in the model (given that the other variables are in the model as well).
- For the test of `Gender` with $H_0: \beta_5 = 0$ VS $H_a: \beta_5 \ne 0$ we obtain $-2log(\Delta) =$ `r round(LR.chisq[5],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq[5],4)`$)=$ `r p.val.chisq[5]` meaning that we have NO evidence that `Gender` is important to include in the model (given that the other variables are in the model as well). Meaning that we would fail to reject $H_0: \beta_5 = 0$.


## Test a hypothesis: Non linear effect (15 Points)

Perform a likelihood ratio test to assess the hypothesis that $\beta_6 = 0$, $\beta_7 = 0$, and $\beta_8 = 0$ within the context of `Model 2`. Interpret the implications of this test result in the context of the estimated `Model 2`.

Then, test the same hypothesis in `Model 3` using a likelihood ratio test. Interpret what this test result means in the context of a model like what you have estimated in ` Model 3`. 

So we have the following hypotheses for both tests:

$H_0: \beta_6 = \beta_7 = \beta_8 = 0$ VS $H_a: \beta_6 \ne 0 \text{ and/or }\beta_7 \ne 0 \text{ and/or } \beta_8 \ne 0$

We can run the first one by comparing `Model 2` with `Model 1` (which is the same as having a model 2 under $H_0$) as follows:

```{r}
mod2.quad.anova <- anova(mod.fit1, mod.fit2, test = "Chisq")
LR.chisq.m2quad <- mod2.quad.anova[["Deviance"]]
p.val.m2quad <- mod2.quad.anova[["Pr(>Chi)"]]
mod2.quad.anova
```

We obtain $-2log(\Delta) =$ `r round(LR.chisq.m2quad[2],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq.m2quad[2],4)`$)=$ `r p.val.m2quad[2]` which means that (under the context of `Model 2`) there is a strong evidence that including the Quadratic Terms is important.

On the other hand to run the second test we need to compare model 3 against a model without the Quadratic terms, as follows:

```{r}
mod.fit3.H0.quad <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen + gender + 
    SeniorCitizen:tenure + SeniorCitizen:MonthlyCharges + SeniorCitizen:TotalCharges + 
    gender:tenure + gender:MonthlyCharges + gender:TotalCharges, 
  family = binomial(link = logit), 
  data = telcom_churn
)

mod3.quad.anova <- anova(mod.fit3.H0.quad, mod.fit3, test = "Chisq")
LR.chisq.m3quad <- mod3.quad.anova[["Deviance"]]
p.val.m3quad <- mod3.quad.anova[["Pr(>Chi)"]]
mod3.quad.anova
```


And now we obtain $-2log(\Delta) =$ `r round(LR.chisq.m3quad[2],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq.m3quad[2],4)`$)=$ `r p.val.m3quad[2]` which means that (under the context of `Model 3`) there is a strong evidence that including the Quadratic Terms is also important.

## Test a hypothesis: Total effect of gender (15 Points)
Test the hypothesis that `gender` has no effect on the likelihood of churn, in  `Model 3`, using a likelihood ratio test. 

We have the following model:
$$
  \begin{aligned}
    Churn = \beta_{0} &+ \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen +\beta_{5} gender \\ 
      & +  \beta_{6} tenure^2 + \beta_{7} MonthlyCharges^2 + \beta_{8} TotalCharges^2 \\ 
      & + {\beta}_9 SeniorCitizen \times tenure + {\beta}_{10} SeniorCitizen \times MonthlyCharges \\ 
      & + \beta_{11} SeniorCitizen \times TotalCharges+ {\beta}_{12} gender \times tenure \\
      & + {\beta}_{13} gender \times MonthlyCharges + \beta_{14} gender \times TotalCharges      + e
  \end{aligned}
$$

To test this hypothesis under `Model 3` we could just use the `gender` term by itself to test its specific impact in the model, and have the following test: $H_0: \beta_5 = 0$ VS $H_a: \beta_5 \ne 0$

```{r}
mod.fit3.H0.gen <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen +
    I(tenure^2) + I(MonthlyCharges^2) + I(TotalCharges^2) + SeniorCitizen:tenure + 
    SeniorCitizen:MonthlyCharges + SeniorCitizen:TotalCharges + gender:tenure + 
    gender:MonthlyCharges + gender:TotalCharges, 
  family = binomial(link = logit), 
  data = telcom_churn
)

mod3.gen.anova <- anova(mod.fit3.H0.gen, mod.fit3, test = "Chisq")
LR.chisq.m3gen <- mod3.gen.anova[["Deviance"]]
p.val.m3gen <- mod3.gen.anova[["Pr(>Chi)"]]
mod3.gen.anova
```

And so we obtain $-2log(\Delta) =$ `r round(LR.chisq.m3gen[2],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq.m3gen[2],4)`$)=$ `r p.val.m3gen[2]` which means that (under the context of `Model 3`) there is NO evidence that including the gender term is important. We couldn't reject $H_0: \beta_5 = 0$.

Although, gender is also included in many interaction terms that might be (or not be) important to the model. So if we want to test the FULL impact of `gender` in our model we would have a test: $H_0: \beta_5 = \beta_{12} = \beta_{13} = \beta_{14} = 0$ VS $H_a: \beta_5 \ne 0 \text{ and/or }\beta_{12} \ne 0 \text{ and/or } \beta_{13} \ne 0 \text{ and/or } \beta_{14} \ne 0$

```{r}
mod.fit3.H0.nogen <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen +
    I(tenure^2) + I(MonthlyCharges^2) + I(TotalCharges^2) + SeniorCitizen:tenure + 
    SeniorCitizen:MonthlyCharges + SeniorCitizen:TotalCharges, 
  family = binomial(link = logit), 
  data = telcom_churn
)

mod3.nogen.anova <- anova(mod.fit3.H0.nogen, mod.fit3, test = "Chisq")
LR.chisq.m3nogen <- mod3.nogen.anova[["Deviance"]]
p.val.m3nogen <- mod3.nogen.anova[["Pr(>Chi)"]]
mod3.nogen.anova
```

And so we have $-2log(\Delta) =$ `r round(LR.chisq.m3nogen[2],4)` and a p-value of $\mathbb{P}(A>$ `r round(LR.chisq.m3nogen[2],4)`$)=$ `r p.val.m3nogen[2]` which means that (under the context of `Model 3`) there is marginal evidence that including all of these terms might be important (depending on the chosen $\alpha$),

## Senior V.S. non-senior customers (20 Points)

Estimate a new model, `Model 4`, by excluding all insignificant variables from `Model 3`. Then, predict how the likelihood of churn changes for senior customers compared to non-senior customers, while keeping `tenure,` `MonthlyCharges,` and `TotalCharges` at their average values. 

To do this we can perform a LRT as follows.

```{r}
Anova(mod.fit3)
```

These results are consistent with Table 1. So, taking any test and $\alpha = 0.05$ we could use the following model:

$$
  \begin{aligned}
    Model_4: Churn = \beta_{0} &+ \beta_{1} tenure + \beta_{2} MonthlyCharges +\beta_{3} TotalCharges + \beta_{4} SeniorCitizen\\ 
      & +  \beta_{5} tenure^2 + \beta_{6} TotalCharges^2 + {\beta}_{7} SeniorCitizen \times MonthlyCharges  + e
  \end{aligned}
$$

```{r}
mod.fit4 <- glm(
  formula = Churn ~ tenure + MonthlyCharges + TotalCharges + SeniorCitizen + 
    I(tenure^2) + I(TotalCharges^2) + SeniorCitizen:MonthlyCharges,
  family = binomial(link = logit), 
  data = telcom_churn
)
```

Then, we can compute the averages and build two dataframes (one for Seniors, another for Non-Seniors) as follows:

```{r}
avg_ten <- mean(telcom_churn$tenure)
avg_Mon <- mean(telcom_churn$MonthlyCharges)
avg_Tot <- mean(telcom_churn$TotalCharges)

predict.data.sen <- data.frame(
  tenure = avg_ten, 
  MonthlyCharges = avg_Mon, 
  TotalCharges = avg_Tot, 
  SeniorCitizen = "Yes"
)

predict.data.nonsen <- data.frame(
  tenure = avg_ten, 
  MonthlyCharges = avg_Mon, 
  TotalCharges = avg_Tot, 
  SeniorCitizen = "No"
)
```

Finally, we can compute the probability of churning for both groups:

```{r}
predict.sen <- predict(
  object = mod.fit4, 
  newdata = predict.data.sen, 
  type = "link", 
  se = TRUE
)
pi.hat_sen <- exp(predict.sen$fit) / (1 + exp(predict.sen$fit))
pi.hat_sen

predict.nonsen <- predict(
  object = mod.fit4, 
  newdata = predict.data.nonsen, 
  type = "link", 
  se = TRUE
)
pi.hat_nonsen <- exp(predict.nonsen$fit) / (1 + exp(predict.nonsen$fit))
pi.hat_nonsen
```

So we have the probability of churning:

- For Seniors: $\hat{\pi}_{Senior} =$ `r round(pi.hat_sen*100,2)`%.
- For Non-Seniors: $\hat{\pi}_{NonSenior} =$ `r round(pi.hat_nonsen*100,2)`% 

When keeping `tenure,` `MonthlyCharges,` and `TotalCharges` at their average values. 

## Construct a confidence interval (20 Points)
Use `Model 4` and construct the 95% wald confidence interval for the churn probability for the customers with the following profile: 

- $tenure  = 55.00$; 
- $MonthlyCharges  =  89.86$; 
- $TotalCharges = 3794.7$;
- $SeniorCitizen = "No"$;

```{r}
predict.data1 <- data.frame(
  tenure = 55.00, 
  MonthlyCharges = 89.86, 
  TotalCharges = 3794.7, 
  SeniorCitizen = "No"
)
linear.pred1 <- predict(object = mod.fit4, newdata = predict.data1, type = "link", se = TRUE)
pi.hat1 <- exp(linear.pred1$fit) / (1 + exp(linear.pred1$fit))
CI.lin.pred1 <- linear.pred1$fit + qnorm(p = c(0.05/2, 1 - 0.05/2)) * linear.pred1$se
CI.pi1 <- exp(CI.lin.pred1) / (1+exp(CI.lin.pred1))
round(data.frame(pi.hat1, lower = CI.pi1[1], upper = CI.pi1[2]),4)
```

Meaning, that for this profile we have a $\hat{\pi} =$ `r round(pi.hat1*100,2)`% and a CI of `r round(CI.pi1[1],4)` $< \hat{\pi} <$ `r round(CI.pi1[2],4)`

and

- $tenure  = 29.00$; 
- $MonthlyCharges  = 18.25$;
- $TotalCharges = 401.4$;
- $SeniorCitizen = "Yes"$

```{r}
predict.data2 <- data.frame(
  tenure = 29.00, 
  MonthlyCharges = 18.25, 
  TotalCharges = 401.4, 
  SeniorCitizen = "Yes"
)
linear.pred2 <- predict(object = mod.fit4, newdata = predict.data2, type = "link", se = TRUE)
pi.hat2 <- exp(linear.pred2$fit) / (1 + exp(linear.pred2$fit))
CI.lin.pred2 <- linear.pred2$fit + qnorm(p = c(0.05/2, 1 - 0.05/2)) * linear.pred2$se
CI.pi2 <- exp(CI.lin.pred2) / (1+exp(CI.lin.pred2))
round(data.frame(pi.hat2, lower = CI.pi2[1], upper = CI.pi2[2]),4)
```

And for this profile we have a $\hat{\pi} =$ `r round(pi.hat2*100,2)`% and a CI of `r round(CI.pi2[1],4)` $< \hat{\pi} <$ `r round(CI.pi2[2],4)`











